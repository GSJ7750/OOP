{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url = 'https://www.bbc.com/korean/news-46562902'\n",
    "r = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = soup.find(class_=\"story-body__introduction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = mr.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_text = list(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacing = []\n",
    "for i in range(len(split_text)):\n",
    "    if split_text[i] == ' ':\n",
    "        spacing.append(([split_text[i-1], split_text[i+1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['늘', '밤'],\n",
       " [',', '쌍'],\n",
       " ['리', '유'],\n",
       " ['가', '쏟'],\n",
       " ['서', '2'],\n",
       " ['의', '가'],\n",
       " ['장', '화'],\n",
       " ['한', '우'],\n",
       " ['가', '밤'],\n",
       " ['에', '펼'],\n",
       " ['질', '것'],\n",
       " ['로', '보']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "split1 = split_text\n",
    "df = pd.get_dummies(split1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for j in range(44):\n",
    "    if split_text[j] == ' ':\n",
    "        print(j)\n",
    "        split_text.pop(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_text = list(filter((' ').__ne__, split_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "split2 = split_text\n",
    "df2 = pd.get_dummies(split2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47, 42)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata = df2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.ones([xdata.shape[0], 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "8\n",
      "12\n",
      "17\n",
      "23\n",
      "25\n",
      "28\n",
      "32\n",
      "36\n",
      "39\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "for k in range(len(spacing)):\n",
    "    for l in range(len(split2)):\n",
    "        if split2[l] == spacing[k][0]:\n",
    "            if split2[l+1] == spacing[k][1]:\n",
    "                print(l)\n",
    "                y[l][0] = y[l][0]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47, 42)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "sess = tf.Session()\n",
    "\n",
    "x = xdata\n",
    "\n",
    "\n",
    "y_ = np.zeros([xdata.shape[0], xdata.shape[0]-1])\n",
    "\n",
    "y = np.concatenate((y, y_),axis = 1)\n",
    "\n",
    "\n",
    "c_ = tf.zeros([xdata.shape[0],xdata.shape[0]])\n",
    "h_ = tf.zeros([xdata.shape[0],xdata.shape[0]])\n",
    "\n",
    "\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, xdata.shape[1]])\n",
    "Y = tf.placeholder(dtype=tf.float32, shape=[None, xdata.shape[0]])\n",
    "W = tf.Variable(tf.random_normal([xdata.shape[0], 1]))*0.5\n",
    "b = tf.Variable(tf.random_normal([1]))\n",
    "\n",
    "\n",
    "#he\n",
    "#el\n",
    "#ll\n",
    "#lo\n",
    "\n",
    "seq_len = len(x) #len(x)cell 갯수, 인풋이 몇 덩어리인지\n",
    "num_units = len(sess.run(c_))  #len(sess.run((c_)))# hiddenlayer\n",
    "\n",
    "\n",
    "class lstm:\n",
    "    def build(c, h):\n",
    "        args = tf.concat((X,h), axis=1)\n",
    "#        print(args)\n",
    "\n",
    "        out_size = 4 * num_units\n",
    "        proj_size = args.shape[-1]\n",
    "#        print(out_size)\n",
    "#        print(proj_size)\n",
    "\n",
    "        weights = tf.ones([proj_size, out_size]) * 0.5\n",
    "#        print(weights)\n",
    "\n",
    "\n",
    "        out = tf.matmul(args, weights)\n",
    "#        print(out)\n",
    "\n",
    "        bias = tf.ones([out_size]) * 0.5\n",
    "#        print(bias)\n",
    "\n",
    "        concat = out + bias\n",
    "#        print(concat)\n",
    "\n",
    "        i, j, f, o = tf.split(concat, 4, 1)\n",
    "#        print(i)\n",
    "#        print(j)\n",
    "#        print(f)\n",
    "#        print(o)\n",
    "\n",
    "        g = tf.tanh(j)\n",
    "#        print(g)\n",
    "\n",
    "        def sigmoid_array(x):\n",
    "            return 1 / (1 + tf.exp(-x))\n",
    "\n",
    "        forget_bias = 1.0\n",
    "\n",
    "        sigmoid_f = sigmoid_array(f + forget_bias)\n",
    "#        print(sigmoid_f)\n",
    "\n",
    "        sigmoid_array(i) * g\n",
    "\n",
    "        new_c = c * sigmoid_f + sigmoid_array(i) * g\n",
    "#        print(new_c)\n",
    "\n",
    "        new_h = tf.tanh(new_c) * sigmoid_array(o)\n",
    "#        print(new_h)\n",
    "\n",
    "#        print('\\n new_h:',new_h)\n",
    "#        print('\\n new_c',new_c)\n",
    "\n",
    "#        print(res[1].h)\n",
    "#        print(res[1].c)\n",
    "\n",
    "        return new_c, new_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bx = x[::-1]\n",
    "\n",
    "by = y[::-1]\n",
    "\n",
    "\n",
    "\n",
    "bc_ = tf.zeros([xdata.shape[0],xdata.shape[0]])\n",
    "bh_ = tf.zeros([xdata.shape[0],xdata.shape[0]])\n",
    "\n",
    "\n",
    "bX = tf.placeholder(dtype=tf.float32, shape=[None, xdata.shape[1]])\n",
    "bY = tf.placeholder(dtype=tf.float32, shape=[None, xdata.shape[0]])\n",
    "bW = tf.Variable(tf.random_normal([xdata.shape[0], 1]))*0.5\n",
    "bb = tf.Variable(tf.random_normal([1]))\n",
    "\n",
    "class blstm:\n",
    "    def build(c, h):\n",
    "        args = tf.concat((X,h), axis=1)\n",
    "#        print(args)\n",
    "\n",
    "        out_size = 4 * num_units\n",
    "        proj_size = args.shape[-1]\n",
    "#        print(out_size)\n",
    "#        print(proj_size)\n",
    "\n",
    "        weights = tf.ones([proj_size, out_size]) * 0.5\n",
    "#        print(weights)\n",
    "\n",
    "\n",
    "        out = tf.matmul(args, weights)\n",
    "#        print(out)\n",
    "\n",
    "        bias = tf.ones([out_size]) * 0.5\n",
    "#        print(bias)\n",
    "\n",
    "        concat = out + bias\n",
    "#        print(concat)\n",
    "\n",
    "        i, j, f, o = tf.split(concat, 4, 1)\n",
    "#        print(i)\n",
    "#        print(j)\n",
    "#        print(f)\n",
    "#        print(o)\n",
    "\n",
    "        g = tf.tanh(j)\n",
    "#        print(g)\n",
    "\n",
    "        def sigmoid_array(x):\n",
    "            return 1 / (1 + tf.exp(-x))\n",
    "\n",
    "        forget_bias = 1.0\n",
    "\n",
    "        sigmoid_f = sigmoid_array(f + forget_bias)\n",
    "#        print(sigmoid_f)\n",
    "\n",
    "        sigmoid_array(i) * g\n",
    "\n",
    "        new_bc = c * sigmoid_f + sigmoid_array(i) * g\n",
    "#        print(new_c)\n",
    "\n",
    "        new_bh = tf.tanh(new_bc) * sigmoid_array(o)\n",
    "#        print(new_h)\n",
    "\n",
    "#        print('\\n new_h:',new_h)\n",
    "#        print('\\n new_c',new_c)\n",
    "\n",
    "#        print(res[1].h)\n",
    "#        print(res[1].c)\n",
    "\n",
    "        return new_bc, new_bh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########################flstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta_c = tf.TensorArray(size=seq_len, dtype=tf.float32)\n",
    "ta_h = tf.TensorArray(size=seq_len, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def body(last_state, last_output, step, ta_c, ta_h):\n",
    "    \n",
    "    output = lstm.build(last_state, last_output)[0]\n",
    "    state = lstm.build(last_state, last_output)[1]\n",
    "    ta_c = ta_c.write(step, state)\n",
    "    ta_h = ta_h.write(step, output)\n",
    "    return state, output, tf.add(step, 1), ta_c, ta_h\n",
    "    \n",
    "\n",
    "timesteps = seq_len\n",
    "\n",
    "steps = lambda a, b, step, c, d: tf.less(step, timesteps)\n",
    "\n",
    "lstm_output, lstm_state, step, ta_c, ta_h = tf.while_loop(steps, body, (c_, h_, 0, ta_c, ta_h), parallel_iterations=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = lstm_output\n",
    "logits = tf.matmul(output, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'mean_square_error_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.name_scope('mean_square_error'):\n",
    "    mean_square_error = tf.reduce_sum(tf.square(tf.subtract(Y, tf.unstack(logits, axis = 1))))\n",
    "tf.summary.scalar('mean_square_error', mean_square_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(0.0003)\n",
    "minimize = optimizer.minimize(mean_square_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'error_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.name_scope('error'):\n",
    "    with tf.name_scope('mistakes'):\n",
    "        mistakes = tf.not_equal(Y, tf.round(tf.unstack(logits, axis = 1)))\n",
    "    with tf.name_scope('error'):\n",
    "        error = tf.reduce_mean(tf.cast(mistakes, tf.float32))\n",
    "tf.summary.scalar('error', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "date = str(datetime.datetime.now())\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  100 | incorrect  100.0% | mean squ error  11097.0\n",
      "Epoch  200 | incorrect  100.0% | mean squ error  6077.0\n",
      "Epoch  300 | incorrect  98.4% | mean squ error  3064.3\n",
      "Epoch  400 | incorrect  98.4% | mean squ error  1414.1\n",
      "Epoch  500 | incorrect  98.4% | mean squ error  599.7\n",
      "Epoch  600 | incorrect  1.6% | mean squ error  241.2\n",
      "Epoch  700 | incorrect  1.6% | mean squ error  101.7\n",
      "Epoch  800 | incorrect  1.6% | mean squ error  53.8\n",
      "Epoch  900 | incorrect  1.6% | mean squ error  39.3\n",
      "Epoch 1000 | incorrect  1.6% | mean squ error  35.5\n",
      "Epoch 1100 | incorrect  1.6% | mean squ error  34.7\n",
      "Epoch 1200 | incorrect  1.6% | mean squ error  34.5\n",
      "Epoch 1300 | incorrect  1.6% | mean squ error  34.5\n",
      "Epoch 1400 | incorrect  1.6% | mean squ error  34.4\n",
      "Epoch 1500 | incorrect  1.6% | mean squ error  34.4\n",
      "Epoch 1600 | incorrect  1.6% | mean squ error  34.4\n",
      "Epoch 1700 | incorrect  1.6% | mean squ error  34.4\n",
      "Epoch 1800 | incorrect  1.6% | mean squ error  34.4\n",
      "Epoch 1900 | incorrect  1.6% | mean squ error  34.4\n",
      "Epoch 2000 | incorrect  1.6% | mean squ error  34.4\n",
      "Epoch 2100 | incorrect  1.6% | mean squ error  34.4\n",
      "Epoch 2200 | incorrect  1.6% | mean squ error  34.4\n",
      "Epoch 2300 | incorrect  1.6% | mean squ error  34.4\n",
      "Epoch 2400 | incorrect  1.6% | mean squ error  34.4\n",
      "Epoch 2500 | incorrect  1.6% | mean squ error  34.4\n",
      "Epoch 2600 | incorrect  1.6% | mean squ error  34.4\n",
      "Epoch 2700 | incorrect  1.6% | mean squ error  34.4\n",
      "Epoch 2800 | incorrect  1.6% | mean squ error  34.4\n",
      "Epoch 2900 | incorrect  1.6% | mean squ error  34.4\n",
      "Epoch 3000 | incorrect  1.6% | mean squ error  34.4\n"
     ]
    }
   ],
   "source": [
    "epoch = 3000\n",
    "\n",
    "for i in range(epoch):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        summary, incorrect, mean_squ_err = sess.run([merged, error, mean_square_error], {X:x, Y:y})\n",
    "        \n",
    "        print('Epoch {:4d} | incorrect {: 3.1f}% | mean squ error {: 3.1f}'.format(i + 1, incorrect * 100, mean_squ_err))\n",
    "    else:\n",
    "        summary, acc = sess.run([merged, error], {X:x, Y:y})\n",
    "\n",
    "\n",
    "    sess.run(minimize,{X:x, Y:y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##################################back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bta_c = tf.TensorArray(size=seq_len, dtype=tf.float32)\n",
    "bta_h = tf.TensorArray(size=seq_len, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbody(last_state, last_output, step, bta_c, bta_h):\n",
    "    \n",
    "    boutput = blstm.build(last_state, last_output)[0]\n",
    "    bstate = blstm.build(last_state, last_output)[1]\n",
    "    bta_c = bta_c.write(step, bstate)\n",
    "    bta_h = bta_h.write(step, boutput)\n",
    "    return bstate, boutput, tf.add(step, 1), bta_c, bta_h\n",
    "    \n",
    "\n",
    "timesteps = seq_len\n",
    "\n",
    "\n",
    "steps = lambda a, b, step, c, d: tf.less(step, timesteps)\n",
    "\n",
    "blstm_output, blstm_state, step, bta_c, bta_h = tf.while_loop(steps, bbody, (bc_, bh_, 0, bta_c, bta_h), parallel_iterations=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "boutput = blstm_output\n",
    "blogits = tf.matmul(boutput, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'mean_square_error_3:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.name_scope('mean_square_error'):\n",
    "    mean_square_error = tf.reduce_sum(tf.square(tf.subtract(Y, tf.unstack(logits, axis = 1))))\n",
    "tf.summary.scalar('mean_square_error', mean_square_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(0.0003)\n",
    "minimize = optimizer.minimize(mean_square_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'error_3:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.name_scope('error'):\n",
    "    with tf.name_scope('mistakes'):\n",
    "        mistakes = tf.not_equal(Y, tf.round(tf.unstack(logits, axis = 1)))\n",
    "    with tf.name_scope('error'):\n",
    "        error = tf.reduce_mean(tf.cast(mistakes, tf.float32))\n",
    "tf.summary.scalar('error', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "date = str(datetime.datetime.now())\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  100 | incorrect  100.0% | mean squ error  66134.0\n",
      "Epoch  200 | incorrect  100.0% | mean squ error  51298.1\n",
      "Epoch  300 | incorrect  100.0% | mean squ error  39145.3\n",
      "Epoch  400 | incorrect  100.0% | mean squ error  29341.6\n",
      "Epoch  500 | incorrect  100.0% | mean squ error  21568.1\n",
      "Epoch  600 | incorrect  100.0% | mean squ error  15520.9\n",
      "Epoch  700 | incorrect  100.0% | mean squ error  10915.5\n",
      "Epoch  800 | incorrect  100.0% | mean squ error  7489.0\n",
      "Epoch  900 | incorrect  100.0% | mean squ error  5004.3\n",
      "Epoch 1000 | incorrect  98.4% | mean squ error  3252.4\n",
      "Epoch 1100 | incorrect  98.4% | mean squ error  2053.8\n",
      "Epoch 1200 | incorrect  98.4% | mean squ error  1260.2\n",
      "Epoch 1300 | incorrect  98.4% | mean squ error  752.7\n",
      "Epoch 1400 | incorrect  1.6% | mean squ error  440.0\n",
      "Epoch 1500 | incorrect  1.6% | mean squ error  254.6\n",
      "Epoch 1600 | incorrect  1.6% | mean squ error  149.2\n",
      "Epoch 1700 | incorrect  1.6% | mean squ error  91.7\n",
      "Epoch 1800 | incorrect  1.6% | mean squ error  61.7\n",
      "Epoch 1900 | incorrect  1.6% | mean squ error  46.9\n",
      "Epoch 2000 | incorrect  1.6% | mean squ error  39.8\n",
      "Epoch 2100 | incorrect  1.6% | mean squ error  36.7\n",
      "Epoch 2200 | incorrect  1.6% | mean squ error  35.3\n",
      "Epoch 2300 | incorrect  1.6% | mean squ error  34.8\n",
      "Epoch 2400 | incorrect  1.6% | mean squ error  34.6\n",
      "Epoch 2500 | incorrect  1.6% | mean squ error  34.5\n",
      "Epoch 2600 | incorrect  1.6% | mean squ error  34.5\n",
      "Epoch 2700 | incorrect  1.6% | mean squ error  34.4\n",
      "Epoch 2800 | incorrect  1.6% | mean squ error  34.4\n",
      "Epoch 2900 | incorrect  1.6% | mean squ error  34.4\n",
      "Epoch 3000 | incorrect  1.6% | mean squ error  34.4\n"
     ]
    }
   ],
   "source": [
    "epoch = 3000\n",
    "\n",
    "for i in range(epoch):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        summary, incorrect, mean_squ_err = sess.run([merged, error, mean_square_error], {X:x, Y:y})\n",
    "        \n",
    "        print('Epoch {:4d} | incorrect {: 3.1f}% | mean squ error {: 3.1f}'.format(i + 1, incorrect * 100, mean_squ_err))\n",
    "    else:\n",
    "        summary, acc = sess.run([merged, error], {X:x, Y:y})\n",
    "\n",
    "\n",
    "    sess.run(minimize,{X:x, Y:y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw = sess.run(tf.equal(sess.run(Y, feed_dict={X:x, Y:y}),sess.run(tf.round(tf.unstack(logits, axis = 1)),feed_dict={X:x, Y:y})))\n",
    "fw = sess.run(tf.one_hot(fw, 1, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw = sess.run(tf.equal(sess.run(Y, feed_dict={X:bx, Y:by}),sess.run(tf.round(tf.unstack(blogits, axis = 1)),feed_dict={X:bx, Y:by})))\n",
    "bw = sess.run(tf.one_hot(bw, 1, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw = fw[0]\n",
    "fw = fw[:,[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw = bw[0]\n",
    "bw = bw[:,[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bi_Lstm_Output = np.column_stack((fw,bw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bi_Lstm_Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = split2\n",
    "s_output = []\n",
    "\n",
    "for i in range(len(sentence)):\n",
    "    s_output.append(sentence[i])\n",
    "    if Bi_Lstm_Output[i][0] == 0:\n",
    "        s_output.append(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오',\n",
       " '늘',\n",
       " ' ',\n",
       " '밤',\n",
       " ',',\n",
       " ' ',\n",
       " '쌍',\n",
       " '둥',\n",
       " '이',\n",
       " '자',\n",
       " '리',\n",
       " ' ',\n",
       " '유',\n",
       " '성',\n",
       " '우',\n",
       " '가',\n",
       " ' ',\n",
       " '쏟',\n",
       " '아',\n",
       " '지',\n",
       " '면',\n",
       " '서',\n",
       " ' ',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '8',\n",
       " '년',\n",
       " '의',\n",
       " ' ',\n",
       " '가',\n",
       " '장',\n",
       " ' ',\n",
       " '화',\n",
       " '려',\n",
       " '한',\n",
       " ' ',\n",
       " '우',\n",
       " '주',\n",
       " '쇼',\n",
       " '가',\n",
       " ' ',\n",
       " '밤',\n",
       " '하',\n",
       " '늘',\n",
       " '에',\n",
       " ' ',\n",
       " '펼',\n",
       " '쳐',\n",
       " '질',\n",
       " ' ',\n",
       " '것',\n",
       " '으',\n",
       " '로',\n",
       " ' ',\n",
       " '보',\n",
       " '인',\n",
       " '다',\n",
       " '.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_output = []\n",
    "\n",
    "for i in range(len(sentence)):\n",
    "    bs_output.append(sentence[i])\n",
    "    if Bi_Lstm_Output[-i-1][1] == 0:\n",
    "        bs_output.append(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오',\n",
       " '늘',\n",
       " ' ',\n",
       " '밤',\n",
       " ',',\n",
       " ' ',\n",
       " '쌍',\n",
       " '둥',\n",
       " '이',\n",
       " '자',\n",
       " '리',\n",
       " ' ',\n",
       " '유',\n",
       " '성',\n",
       " '우',\n",
       " '가',\n",
       " ' ',\n",
       " '쏟',\n",
       " '아',\n",
       " '지',\n",
       " '면',\n",
       " '서',\n",
       " ' ',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '8',\n",
       " '년',\n",
       " '의',\n",
       " ' ',\n",
       " '가',\n",
       " '장',\n",
       " ' ',\n",
       " '화',\n",
       " '려',\n",
       " '한',\n",
       " ' ',\n",
       " '우',\n",
       " '주',\n",
       " '쇼',\n",
       " '가',\n",
       " ' ',\n",
       " '밤',\n",
       " '하',\n",
       " '늘',\n",
       " '에',\n",
       " ' ',\n",
       " '펼',\n",
       " '쳐',\n",
       " '질',\n",
       " ' ',\n",
       " '것',\n",
       " '으',\n",
       " '로',\n",
       " ' ',\n",
       " '보',\n",
       " '인',\n",
       " '다',\n",
       " '.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf1.8]",
   "language": "python",
   "name": "conda-env-tf1.8-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
